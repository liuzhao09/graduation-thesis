\chapter{大小模型协同学习方法}
\label{chap:method}
\newcommand{\cThreeTblStyle}{\centering\zihao{5}\songti\rmfamily\setlength{\tabcolsep}{4.8pt}\renewcommand{\arraystretch}{1.15}}
\newcommand{\YesMark}{\ding{51}}

\section{本章引言}
基于第\ref{chap:related}章给出的研究空白与建模原则，本章给出本文方法的完整实现方案，重点回答“整体框架如何搭建、关键模块如何工作、训练与推理如何落地”三个问题。为保证论证清晰，本章按小模型分支、大模型分支和协同训练策略依次展开，并将每个模块与对应研究问题（RQ）建立映射关系，从而为第\ref{chap:exp}章的实验验证提供可追溯的机制假设。本文方法综合继承了时间增强序列预测模型（Time-enhanced Sequential Prediction Model, TSPM）与地理感知大语言模型（Geography-Aware Large Language Model, GA-LLM）系列相关工作的有效设计思想\cite{liu2025tspm,liu2026gallm}，并在毕业论文中统一为“双路线并行、训练协同、推理独立”的研究框架。

\section{总体框架与设计动机}
基于第\ref{chap:related}章的问题分析，本文方法由三个部分构成：
\begin{enumerate}
    \item 小模型分支 TSPM：学习时间敏感的时空转移结构\cite{liu2025tspm};
    \item 大模型分支 GA-LLM：增强地理连续性建模与POI先验注入\cite{liu2026gallm};
    \item 对齐/协同训练分支：通过嵌入对齐与两阶段训练实现协同优化。
\end{enumerate}

设计动机是将小模型的结构归纳偏置与大模型的语义推理能力进行互补融合，避免单一路线在精度、鲁棒性或泛化能力上的短板。序列/图模型在局部转移建模上具有优势\cite{GETNext,STHGCN,ROTAN,MTNet}，而LLM在语义迁移与复杂上下文理解上更强\cite{LLM4POI,LLaRA,SeCor}；本文的关键是构建一条低损耗的信息通道，使二者不再相互替代，而是协同增益。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{fig3_framework.pdf}
    \bicaption{大小模型协同框架示意：图中重点展示GA-LLM主推理链路，以及GCIM与PAM在输入侧的注入流程；其中PAM用于承接结构侧（含小模型）学习到的POI关系信息并映射到LLM语义空间，最终Top-$K$由GA-LLM单路输出。}{Illustration of the collaborative framework: the figure emphasizes the GA-LLM inference path and the input-side injection of GCIM and PAM. PAM bridges structure-side knowledge (including small-model learned POI relations) into the LLM semantic space, while final Top-$K$ predictions are generated by GA-LLM only.}
    \label{fig:c3-framework}
\end{figure}

如图\ref{fig:c3-framework}所示，图示聚焦“语义分支如何吸收结构先验并完成推理输出”的关键路径：轨迹输入先经GCIM形成地理编码，再由PAM注入结构侧关系信息，随后由GA-LLM完成下一POI生成。该结构并不表示TSPM直接参与在线推理，而是强调PAM作为跨分支接口承接协同信息。该设计对应第\ref{chap:exp}章RQ1与RQ4，用于验证结构信息注入带来的整体收益与互补性。

\subsection{输入构建与数据流定义}
为便于实现与复现，本文将输入组织为三类并行流：
\begin{enumerate}
    \item {\heiti\addCJKfontfeatures{AutoFakeBold=2}序列流：}由时间有序POI序列构成，用于TSPM提取局部与短期迁移模式；
    \item {\heiti\addCJKfontfeatures{AutoFakeBold=2}结构流：}由动态图邻接关系构成，用于建模高阶转移与区域约束；
    \item {\heiti\addCJKfontfeatures{AutoFakeBold=2}语义流：}由轨迹文本、POI语义token与地理token构成，用于GA-LLM进行语义推理。
\end{enumerate}
三类输入在特征层保持相对独立，在损失层进行耦合，在推理层保持GA-LLM单路输出。该设计可避免“早期强融合导致互相干扰”，并降低训练初期梯度不稳定风险。

\subsection{核心符号与问题映射}
本章沿用第\ref{chap:related}章给出的统一符号体系，不再重复列出完整符号表。为便于阅读，本章仅补充实现级新增变量：$\boldsymbol{\xi}^{out},\boldsymbol{\xi}^{in}$（双向转移表示）、$\boldsymbol{\xi}_{seq}$（序列聚合表示）、$\mathbf{E}_{gps}$（地理编码表示）、$\mathbf{E}_{poi}$（POI对齐表示）、$\mathcal{L}_{time},\mathcal{L}_{seq},\mathcal{L}_{geo},\mathcal{L}_{align},\mathcal{L}_{gen}$（各模块损失项）。其余符号均与第\ref{chap:related}章定义保持一致。

\subsection{协同设计原则}
本文在框架层遵循三条工程原则：
\begin{enumerate}
    \item {\heiti\addCJKfontfeatures{AutoFakeBold=2}先对齐后协同：}先保证不同空间表示可互相读取，再进行联合优化；
    \item {\heiti\addCJKfontfeatures{AutoFakeBold=2}模块可插拔：}GCIM、PAM与TSPM均可独立启停，便于消融诊断；
    \item {\heiti\addCJKfontfeatures{AutoFakeBold=2}训练资源可控：}大模型侧以参数高效微调为主，避免全参更新带来的成本激增。
\end{enumerate}

\section{关键模块设计}
\subsection{小模型分支：TSPM}
\subsubsection{初始嵌入构建}
TSPM分支首先构建可训练的POI初始表示，以保证后续时间分槽与动态图计算有稳定输入。本文采用“关系旋转表示 + 局部拓扑保持”的组合初始化：前者建模转移关系方向性，后者保持近邻几何结构\cite{sun2019rotate,belkin2003laplacian}。
\begin{equation}
\mathbf{e}^{(0)}_{t}\approx \mathbf{e}^{(0)}_{h}\circ \mathbf{r}_{(h,t)},
\label{eq:c3-init-rotate}
\end{equation}
式中：\symline{$\mathbf{e}^{(0)}_{h},\mathbf{e}^{(0)}_{t}$}{初始阶段的头/尾POI向量；\\}
\hphantom{式中：}\symline{$\mathbf{r}_{(h,t)}$}{POI转移关系向量；\\}
\hphantom{式中：}\symline{$\circ$}{复向量旋转对应的逐元素组合运算。}

\begin{equation}
\mathbf{e}^{(0)}_{i}=\text{EigenMap}(\mathcal{N}(i)),
\label{eq:c3-init-eigen}
\end{equation}
式中：\symline{$\mathbf{e}^{(0)}_{i}$}{POI $i$ 的拓扑保持初始化向量；\\}
\hphantom{式中：}\symline{$\mathcal{N}(i)$}{POI $i$ 的局部邻接结构。}

该初始化的作用是减少“随机初始化导致的早期训练震荡”，并为后续TSDG边权学习提供更平滑的优化起点；在实现上，本文将关系建模与局部几何保持联合使用，以兼顾转移方向性与邻域结构稳定性。

\subsubsection{时间增强序列动态图（TSDG）}
TSDG 中的双向转移部分记为双向转移建模（Bidirectional Transformation Modeling, BTM）。其核心目标是同时学习“转出偏好”和“转入偏好”，与第\ref{chap:exp}章 \textit{w/o BTM} 消融设置一一对应。
为刻画不同时段的迁移差异，将一天划分为 $z$ 个时间槽 $\{T_1,\ldots,T_z\}$，并在各时间槽内构建POI转移子图。对当前POI嵌入 $\mathbf{e}_i$ 与时间槽嵌入 $\mathbf{t}_i$，时间感知转出表示定义为：
\begin{equation}
\boldsymbol{\xi}^{out}_{i,T_i}=\sigma\left([\mathbf{e}_i\Vert \mathbf{t}_i]\mathbf{W}^{t}_{out}+\mathbf{b}^{t}_{out}\right),
\label{eq:c3-xi-out}
\end{equation}
式中：\symline{$\boldsymbol{\xi}^{out}_{i,T_i}$}{POI $i$ 在时间槽 $T_i$ 的转出表示；\\}
\hphantom{式中：}\symline{$[\mathbf{e}_i\Vert \mathbf{t}_i]$}{POI向量与时间槽向量拼接；\\}
\hphantom{式中：}\symline{$\mathbf{W}^{t}_{out},\mathbf{b}^{t}_{out}$}{转出分支参数；\\}
\hphantom{式中：}\symline{$\sigma(\cdot)$}{非线性激活函数。}

时间感知转入表示定义为：
\begin{equation}
\boldsymbol{\xi}^{in}_{j,T_i}=\sigma\left([\mathbf{e}_j\Vert \mathbf{t}_i]\mathbf{W}^{t}_{in}+\mathbf{b}^{t}_{in}\right).
\label{eq:c3-xi-in}
\end{equation}
式中：\symline{$\boldsymbol{\xi}^{in}_{j,T_i}$}{POI $j$ 在时间槽 $T_i$ 的转入表示；\\}
\hphantom{式中：}\symline{$\mathbf{W}^{t}_{in},\mathbf{b}^{t}_{in}$}{转入分支参数；\\}
其余符号与式\eqref{eq:c3-xi-out}一致。

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.43\textwidth}
        \centering
        \includegraphics[height=0.16\textheight,keepaspectratio]{Time_enhanced_left.pdf}
        \caption{传统连续序列}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.53\textwidth}
        \centering
        \includegraphics[height=0.16\textheight,keepaspectratio]{Time_enhanced_right.pdf}
        \caption{时间增强序列}
    \end{subfigure}
    \bicaption{传统序列与时间增强序列的建模对比。}{Modeling comparison between the conventional sequential setting and the time-enhanced sequential setting.}
    \label{fig:c3-time-compare}
\end{figure}

如图\ref{fig:c3-time-compare}所示，传统序列将不同时间段的迁移关系混合建模，而时间增强序列能显式区分时段特征。该现象说明“同一POI在不同时段具有不同转移分布”是必须建模的结构事实。该结论直接回扣本节TSDG设计，并将在第\ref{chap:exp}章RQ2中通过消融实验验证。

{\heiti\addCJKfontfeatures{AutoFakeBold=2}待验证命题：}显式时间分槽可提升模型对时段异质行为的建模能力（对应第\ref{chap:exp}章RQ2）。

\topichead{时间分槽策略讨论}
时间分槽粒度过粗会掩盖行为差异，过细会造成样本稀疏。本文采用“以行为节律为导向”的折中策略，即优先保证每个时段具有足够样本密度，再在验证集上微调分槽数量。该策略相比固定等距切分更贴近真实城市活动节奏。

\subsubsection{双向转移建模}
仅建模“转出”（当前点通常去哪里）在若干场景下会出现歧义。以工作日早高峰为例：用户从地铁站出发时，候选目的地可能同时包含写字楼、早餐店与便利店；如果模型只看“地铁站的常见去向”，容易被高频近邻点吸引而忽略真实通勤目的地。反过来看“转入”特性可提供额外判别信息：写字楼在该时段通常承接大量来自交通枢纽的入流，而便利店的入流更分散且短停留。再如晚间场景，商场与住宅区可能共享相似语义标签，但住宅区在夜间具有更稳定的“被到达”模式。  

基于上述现象，本文同时建模“从哪里来”和“将去哪里”，即联合学习转出表示与转入表示，并通过双向对比损失约束二者的一致性：
\begin{equation}
\mathcal{L}_{time}=-\sum_t\log\sigma\left(\|\boldsymbol{\xi}^{out}_{i,T}-\boldsymbol{\xi}^{in}_{-,T}\|_2^2-\|\boldsymbol{\xi}^{out}_{i,T}-\boldsymbol{\xi}^{in}_{+,T}\|_2^2\right),
\label{eq:c3-ltime}
\end{equation}
式中：\symline{$\mathcal{L}_{time}$}{时间转移损失；\\}
\hphantom{式中：}\symline{$\boldsymbol{\xi}^{in}_{+,T},\boldsymbol{\xi}^{in}_{-,T}$}{分别表示正负样本的转入表示；\\}
\hphantom{式中：}\symline{$\|\cdot\|_2^2$}{平方欧氏距离；\\}
\hphantom{式中：}\symline{$+$ 与 $-$}{分别表示正负样本POI。}

{\heiti\addCJKfontfeatures{AutoFakeBold=2}待验证命题：}双向转移优于单向转移，可减少路径偏置并提升下一跳预测稳定性（对应RQ2）。

\topichead{损失函数直观解释}
式\eqref{eq:c3-ltime}的核心是将“当前点到真实下一点”的距离压缩，同时将“当前点到负样本点”的距离拉开。与单向建模相比，双向表示可同时约束“出边合理性”和“入边合理性”：前者回答“当前点一般指向哪些候选”，后者回答“目标点通常由哪些来源到达”。这种双侧约束可显著减少“高频近邻误吸附”与“语义相似点混淆”，尤其在通勤、跨区跳转与晚高峰回流等方向性强的轨迹中更稳定。

\subsubsection{序列偏好建模与动态图权重}
将最近 $k$ 个访问拼接得到序列表示：
\begin{equation}
\boldsymbol{\xi}_{seq}=\sigma\left([\mathbf{e}_t\Vert\mathbf{e}_{t-1}\Vert\cdots\Vert\mathbf{e}_{t-k}]\mathbf{W}^{s}+\mathbf{b}^{s}\right),
\label{eq:c3-xi-seq}
\end{equation}
式中：\symline{$\boldsymbol{\xi}_{seq}$}{历史序列聚合表示；\\}
\hphantom{式中：}\symline{$k$}{历史窗口长度；\\}
\hphantom{式中：}\symline{$\mathbf{W}^{s},\mathbf{b}^{s}$}{序列映射参数。}

并使用序列对比损失：
\begin{equation}
\mathcal{L}_{seq}=-\sum_t\log\sigma\left(\|\boldsymbol{\xi}_{seq}-\mathbf{e}^{-}_t\|_2^2-\|\boldsymbol{\xi}_{seq}-\mathbf{e}^{+}_{t+1}\|_2^2\right),
\label{eq:c3-lseq}
\end{equation}
式中：\symline{$\mathcal{L}_{seq}$}{序列对比损失；\\}
\hphantom{式中：}\symline{$\mathbf{e}^{+}_{t+1}$}{真实下一POI嵌入；\\}
\hphantom{式中：}\symline{$\mathbf{e}^{-}_{t}$}{负样本POI嵌入。}

综合损失写为：
\begin{equation}
\mathcal{L}_{TSPM}=\alpha\mathcal{L}_{time}+\beta\mathcal{L}_{seq}.
\label{eq:c3-ltspm}
\end{equation}
式中：\symline{$\mathcal{L}_{TSPM}$}{小模型总损失；\\}
\hphantom{式中：}\symline{$\alpha,\beta$}{两部分损失权重。}

据此定义动态图边权：
\begin{equation}
s^d_{i,j}=\exp\left(-\rho_1\|\boldsymbol{\xi}_{seq}-\mathbf{e}_j\|_2^2-\rho_2\|\boldsymbol{\xi}^{out}_{i,T}-\boldsymbol{\xi}^{in}_{j,T}\|_2^2\right).
\label{eq:c3-edgew}
\end{equation}
式中：\symline{$s^d_{i,j}$}{动态图从POI $i$ 到POI $j$ 的边权；\\}
\hphantom{式中：}\symline{$\rho_1,\rho_2$}{两类距离项的平衡系数；\\}
\hphantom{式中：}\symline{$\exp(\cdot)$}{指数映射函数。}

{\heiti\addCJKfontfeatures{AutoFakeBold=2}待验证命题：}动态图权重可提升复杂迁移场景下的区分能力（对应RQ2、RQ4）。

\topichead{负采样策略}
为避免训练信号过于简单，本文采用“同区域困难负样本 + 跨区域随机负样本”的混合策略：前者提升局部细粒度区分难度，后者维持全局判别边界。该策略有助于提升模型在近邻候选上的排序精度，并降低模型仅记忆区域标签的风险。

\subsubsection{TiRNN预测头}
为建模多步历史依赖，本文在预测头引入时间感知循环神经网络（Time-aware Recurrent Neural Network, TiRNN）。其结构如图\ref{fig:c3-tirnn}所示：右侧为历史隐状态序列 $\{\mathbf{h}_{t-1},\ldots,\mathbf{h}_{t-k}\}$，左侧为关系向量 $\{\mathbf{r}_1,\ldots,\mathbf{r}_k\}$；二者先通过逐步旋转交互，再由注意力模块选择关键依赖，最后与当前输入的全连接映射结果做加和，得到当前隐状态 $\mathbf{h}_t$。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.62\textwidth]{TiRNN.pdf}
    \bicaption{TiRNN预测头结构示意图。}{Architecture of the TiRNN prediction head.}
    \label{fig:c3-tirnn}
\end{figure}

该结构的核心作用是把“历史关系建模”和“当前输入建模”分开处理后再融合：注意力分支负责筛选有效历史信号，FC分支负责保留当前时刻的即时偏好，二者相加后可在短期惯性与中期计划之间做自适应平衡。基于该流程，TiRNN对过去 $K$ 个隐状态加权融合：
\begin{equation}
\mathbf{c}_t=\sum_{k=1}^{K}\alpha_k(\mathbf{h}_{t-k}\circ\mathbf{r}_k),
\label{eq:c3-context}
\end{equation}
式中：\symline{$\mathbf{c}_t$}{时刻 $t$ 的历史上下文向量；\\}
\hphantom{式中：}\symline{$K$}{回看步数；\\}
\hphantom{式中：}\symline{$\alpha_k$}{第 $k$ 步历史权重；\\}
\hphantom{式中：}\symline{$\mathbf{h}_{t-k}$}{第 $t-k$ 时刻隐状态；\\}
\hphantom{式中：}\symline{$\circ$}{Hadamard逐元素乘。}

\begin{equation}
\mathbf{h}_t=\sigma(\kappa\mathbf{v}_t+\mathbf{c}_t),\qquad
\hat{\mathbf{y}}_t=\text{Softmax}(\mathbf{W}_f[\hat{\mathbf{h}}_t\Vert\mathbf{E}_u]).
\label{eq:c3-tirnn}
\end{equation}
式中：\symline{$\mathbf{h}_t$}{当前隐状态；\\}
\hphantom{式中：}\symline{$\kappa$}{当前输入权重；\\}
\hphantom{式中：}\symline{$\mathbf{v}_t$}{当前访问表示；\\}
\hphantom{式中：}\symline{$\hat{\mathbf{y}}_t$}{候选POI概率分布；\\}
\hphantom{式中：}\symline{$\mathbf{W}_f$}{预测层参数；\\}
\hphantom{式中：}\symline{$\mathbf{E}_u$}{用户嵌入。}

\topichead{预测头设计动机}
TiRNN并非替代前述结构模块，而是作为“时序聚合终端”整合动态图编码结果。通过显式聚合多步隐状态，模型可以在“短期活动惯性”和“中期出行计划”之间动态权衡，从而减少单步过拟合问题。

\subsection{大模型分支：GA-LLM}
\subsubsection{图3-1中的GA-LLM信息流解读}
为避免“模块分开写、读者难以对齐图示”的问题，这里先按图\ref{fig:c3-framework}给出GA-LLM分支的完整数据流。图中右侧语义分支可拆解为四步：第一步，轨迹事件被组织为统一模板文本；第二步，GCIM将坐标字段编码为地理token并注入输入序列；第三步，PAM将结构侧POI表示映射为语义token并注入同一序列；第四步，LLM直接输出候选分布与Top-$K$结果。后续各小节均对应这四步中的一个关键环节，读者可直接对照图\ref{fig:c3-framework}中“语义流$\rightarrow$对齐$\rightarrow$输出”的路径理解实现细节。

\subsubsection{GCIM：地理坐标注入模块}
GCIM采用“层级离散 + 连续频域”双分支编码。为与实验消融项保持一致，本文将连续频域分支记为连续空间编码（Continuous Spatial Encoding, CSE），将层级离散分支记为层级离散编码（Hierarchical Discrete Encoding, HDE）。其中，CSE定义为：
\begin{equation}
\mathbf{E}_{fourier}=\frac{1}{\sqrt{M}}\left[\cos(\mathbf{g}\mathbf{W}_s^\top)\,\Vert\,\sin(\mathbf{g}\mathbf{W}_s^\top)\right],
\label{eq:c3-fourier}
\end{equation}
式中：\symline{$\mathbf{E}_{fourier}$}{Fourier频域编码向量；\\}
\hphantom{式中：}\symline{$\mathbf{g}$}{坐标输入向量；\\}
\hphantom{式中：}\symline{$\mathbf{W}_s$}{频域投影矩阵；\\}
\hphantom{式中：}\symline{$M$}{归一化维度系数。}

融合地理表示定义为：
\begin{equation}
\mathbf{E}_{gps}=\mathbf{W}_{gps}[\mathbf{E}_{quad}\Vert\mathbf{E}_{fourier}].
\label{eq:c3-egps}
\end{equation}
式中：\symline{$\mathbf{E}_{gps}$}{最终地理编码；\\}
\hphantom{式中：}\symline{$\mathbf{E}_{quad}$}{Quadkey层级编码；\\}
\hphantom{式中：}\symline{$\mathbf{W}_{gps}$}{融合投影参数。}

为进一步约束地理表示与真实测地距离的一致性，本文在训练阶段引入测地对齐损失（Geodesic Alignment Loss, GAL）：
\begin{equation}
\mathcal{L}_{geo}=\frac{1}{|\mathcal{P}|}\sum_{(i,j)\in\mathcal{P}}\left|\operatorname{dist}_{geo}(i,j)-\operatorname{dist}_{emb}(i,j)\right|,
\label{eq:c3-lgeo}
\end{equation}
式中：\symline{$\mathcal{L}_{geo}$}{地理一致性损失，对应实验中的GAL项；\\}
\hphantom{式中：}\symline{$\mathcal{P}$}{批内POI对集合；\\}
\hphantom{式中：}\symline{$\operatorname{dist}_{geo}(i,j)$}{POI $i,j$ 的真实测地距离；\\}
\hphantom{式中：}\symline{$\operatorname{dist}_{emb}(i,j)$}{编码空间中的距离。}

针对第\ref{chap:intro}章图\ref{fig:c1-gcim-motivation}所示的空间错位问题，GCIM在实现层面通过Quadkey分支编码层级区域结构，通过Fourier分支编码连续距离变化，并在投影层完成统一融合。该设计使地理关系以结构化方式进入LLM输入空间，为后续地理一致性约束提供可优化的表示基础。
对应图\ref{fig:c3-framework}，GCIM位于GA-LLM分支的输入侧，直接作用于轨迹事件中的坐标字段，其输出 $\mathbf{E}_{gps}$ 将作为后续结构化提示中的 \texttt{<GPS>} 语义载体，而不是在LLM输出后再做后处理。

{\heiti\addCJKfontfeatures{AutoFakeBold=2}待验证命题：}GCIM可显著改善地理一致性并降低远跳错误（对应RQ3、RQ4）。

\topichead{为何采用Quadkey + Fourier}
Quadkey适合表达“区域层级归属”，Fourier适合表达“连续坐标变化模式”\cite{GeoSAN,STiSAN,liu2026gallm}。前者提供离散空间结构，后者提供连续距离敏感性，二者互补后可同时保留地理分区稳定性与局部变化分辨率。该设计可避免仅离散编码导致的边界不连续，也避免仅连续编码导致的区域语义缺失。

\subsubsection{PAM：POI对齐模块}
在协同框架中，轨迹语义、地理坐标与图结构表示属于不同模态：轨迹文本位于离散token语义空间，坐标经GCIM后形成地理编码空间，而小模型/图模型输出位于结构表征空间。三类表示若直接拼接，常出现“可用信息无法被同一注意力层稳定读取”的问题，具体表现为结构先验在生成阶段贡献不稳定、跨城迁移时增益波动较大。

因此，在将结构信息注入LLM之前，需要先建立“结构空间 $\rightarrow$ 语义空间”的可学习映射通道，使POI关系先验在语义侧具备可比较、可聚合、可反向优化的表示形式。基于这一需求，本文引入POI对齐模块（PAM），其核心变换定义为：
\begin{equation}
\mathbf{E}_{poi}=\text{PAM}(\mathbf{e}_{poi})=\mathbf{W}_{p}\mathbf{e}_{poi}+\mathbf{b}_{p}.
\label{eq:c3-epoi}
\end{equation}
式中：\symline{$\mathbf{e}_{poi}$}{图模型侧POI嵌入；\\}
\hphantom{式中：}\symline{$\mathbf{E}_{poi}$}{映射后的语义空间POI表示；\\}
\hphantom{式中：}\symline{$\mathbf{W}_{p},\mathbf{b}_{p}$}{PAM映射参数。}

针对第\ref{chap:intro}章图\ref{fig:c1-pam-motivation}所示的转移先验缺失问题，PAM在实现层面构建“结构表征到语义表征”的线性映射通道，并将图模型中的迁移知识注入LLM语义空间。该路径避免了仅靠token共现学习关系的局限，使模型在候选稀疏和目标未显式出现时仍可利用结构先验进行推断。
对应图\ref{fig:c3-framework}，PAM承接小模型/结构侧输出并注入到语义侧输入，承担“跨分支桥接”角色；因此它不是独立预测器，而是连接双分支信息流的关键接口。

{\heiti\addCJKfontfeatures{AutoFakeBold=2}待验证命题：}PAM可提升目标POI未显式出现时的预测能力（对应RQ3、RQ4）。

\topichead{对齐目标}
PAM的目标不是“替代LLM语义”，而是建立一条可学习映射，使图模型中可迁移的结构关系能够在LLM空间中被读取和利用。换言之，PAM承担的是“知识通道”角色，核心价值在于降低跨空间信息损失。

\subsubsection{结构化提示构造}
为避免“只有token占位符、缺少可学习语义上下文”的问题，本文将每条轨迹组织为“自然语言事件描述 + 结构化专用token”的混合输入。核心思路是把签到序列转写为按时间递增的事件流，并在每个事件中显式提供时间、POI语义与地理编码。

\topichead{事件文本化规则}
设用户 $u$ 在时刻 $t_i$ 访问POI $p_i$，将该事件标准化为：
\begin{equation}
\mathcal{E}_i=\texttt{At }t_i\texttt{, user }u\texttt{ visited }\texttt{<POI }p_i\texttt{> with location }\texttt{<GPS }g_i\texttt{>}.
\label{eq:c3-event-text}
\end{equation}
式中：\symline{$\mathcal{E}_i$}{第 $i$ 条轨迹事件的文本化表示；\\}
\hphantom{式中：}\symline{$\texttt{<POI }p_i\texttt{>}$}{由PAM注入的POI结构语义token；\\}
\hphantom{式中：}\symline{$\texttt{<GPS }g_i\texttt{>}$}{由GCIM注入的地理编码token。}

给定历史轨迹 $\{\mathcal{E}_1,\ldots,\mathcal{E}_n\}$，训练与推理均使用同一字段顺序，以减少模板漂移造成的分布偏差。

\topichead{微调问答样本格式}
训练阶段采用“Instruction-Input-Output”监督格式，使模型学习从轨迹事实到下一POI的映射。模板如下：
\begin{promptbox}
\textbf{Instruction:}\\
Predict the next POI based on the user's chronological trajectory.\\[2pt]
\textbf{Input:}\\
User ID: U\_104.\\
Trajectory events:\\
1) At 2024-06-03 08:12, user U\_104 visited \texttt{<POI CoffeeShop\_183>} with location \texttt{<GPS 22.2731,113.5728>}.\\
2) At 2024-06-03 08:47, user U\_104 visited \texttt{<POI Office\_592>} with location \texttt{<GPS 22.2765,113.5881>}.\\
3) At 2024-06-03 12:06, user U\_104 visited \texttt{<POI Canteen\_74>} with location \texttt{<GPS 22.2748,113.5850>}.\\
4) At 2024-06-03 18:21, user U\_104 visited \texttt{<POI Gym\_231>} with location \texttt{<GPS 22.2713,113.5794>}.\\
Question: Which POI will user U\_104 visit next?\\[2pt]
\textbf{Output:}\\
\texttt{<POI Supermarket\_406>}
\end{promptbox}

上述格式的关键是：输入中保留完整时序线索（谁、何时、访问了什么、位于哪里），输出仅监督“下一POI标签”，从而使模型将生成能力集中到下一跳判别任务本身。

\topichead{推理阶段模板}
在线推理时保持与微调同构的模板，仅移除监督答案：
\begin{promptbox}
\textbf{Instruction:} Predict the next POI based on the user's chronological trajectory.\\
\textbf{Input:} User ID + ordered trajectory events (with \texttt{<POI>} and \texttt{<GPS>} tokens).\\
\textbf{Question:} Which POI will this user visit next?\\
\textbf{Output:} [Model prediction]
\end{promptbox}

其中，\texttt{<GPS>} 由GCIM编码、\texttt{<POI>} 由PAM编码。该模板在训练与推理阶段字段一致，可有效降低提示分布漂移并提升生成稳定性。
从图\ref{fig:c3-framework}的角度看，结构化提示就是把“GCIM地理编码结果”和“PAM对齐结果”在输入层做显式合并，使GA-LLM在同一上下文内同时读取地理约束与转移先验。

\subsubsection{生成目标与解码策略}
GA-LLM的训练目标为标准自回归负对数似然：
\begin{equation}
\mathcal{L}_{gen}=-\sum_{t=1}^{m}\log p(y_t\mid y_{<t},\mathbf{X}_{traj},\mathbf{E}_{gps},\mathbf{E}_{poi}),
\label{eq:c3-lgen}
\end{equation}
式中：\symline{$y_t$}{第 $t$ 个生成token；\\}
\hphantom{式中：}\symline{$\mathbf{X}_{traj}$}{轨迹文本序列输入；\\}
\hphantom{式中：}\symline{$\mathbf{E}_{gps}$}{GCIM注入的地理表示；\\}
\hphantom{式中：}\symline{$\mathbf{E}_{poi}$}{PAM注入的POI先验表示。}

为与“下一token预测”机制保持一致，时刻 $t$ 的输出分布可写为：
\begin{equation}
p(y_t\mid y_{<t},\mathbf{X})=\operatorname{Softmax}\!\left(\mathbf{W}_o\mathbf{h}_t+\mathbf{b}_o\right),
\label{eq:c3-next-token}
\end{equation}
式中：\symline{$\mathbf{X}$}{融合了轨迹文本、地理编码与POI先验的输入序列；\\}
\hphantom{式中：}\symline{$\mathbf{h}_t$}{解码器在位置 $t$ 的隐藏状态；\\}
\hphantom{式中：}\symline{$\mathbf{W}_o,\mathbf{b}_o$}{词表投影参数。}

在实现层面，融合输入先构造为：
\begin{equation}
\mathbf{H}^{(0)}=\operatorname{Embed}(\mathbf{X}_{traj})+\mathbf{M}_{gps}\odot\mathbf{E}_{gps}+\mathbf{M}_{poi}\odot\mathbf{E}_{poi},
\label{eq:c3-input-fusion}
\end{equation}
式中：\symline{$\mathbf{H}^{(0)}$}{输入层隐藏表示；\\}
\hphantom{式中：}\symline{$\operatorname{Embed}(\cdot)$}{token嵌入查表；\\}
\hphantom{式中：}\symline{$\mathbf{M}_{gps},\mathbf{M}_{poi}$}{注入位置掩码（mask）；\\}
\hphantom{式中：}\symline{$\odot$}{逐元素乘。}

随后每层Transformer采用标准“注意力 + 前馈”更新。单头缩放点积注意力写为：
\begin{equation}
\operatorname{Attn}(\mathbf{Q},\mathbf{K},\mathbf{V})=\operatorname{Softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_k}}\right)\mathbf{V},
\label{eq:c3-attn}
\end{equation}
式中：\symline{$\mathbf{Q},\mathbf{K},\mathbf{V}$}{查询、键、值矩阵；\\}
\hphantom{式中：}\symline{$d_k$}{键向量维度。}

多头注意力与前馈层定义为：
\begin{equation}
\operatorname{MHA}(\mathbf{H})=\operatorname{Concat}(\text{head}_1,\ldots,\text{head}_h)\mathbf{W}^{O},\quad
\text{head}_j=\operatorname{Attn}(\mathbf{H}\mathbf{W}_j^Q,\mathbf{H}\mathbf{W}_j^K,\mathbf{H}\mathbf{W}_j^V),
\label{eq:c3-mha}
\end{equation}
\begin{equation}
\operatorname{FFN}(\mathbf{H})=\phi(\mathbf{H}\mathbf{W}_1+\mathbf{b}_1)\mathbf{W}_2+\mathbf{b}_2,
\label{eq:c3-ffn}
\end{equation}
式中：\symline{$h$}{注意力头数；\\}
\hphantom{式中：}\symline{$\mathbf{W}_j^Q,\mathbf{W}_j^K,\mathbf{W}_j^V$}{第 $j$ 个头的投影参数；\\}
\hphantom{式中：}\symline{$\mathbf{W}^{O}$}{多头输出投影参数；\\}
\hphantom{式中：}\symline{$\phi(\cdot)$}{非线性激活函数（如SiLU/GELU）。}

当采用LoRA进行参数高效微调时，线性层权重更新可写为：
\begin{equation}
\mathbf{W}'=\mathbf{W}+\Delta\mathbf{W},\qquad \Delta\mathbf{W}=\frac{\alpha}{r}\mathbf{B}\mathbf{A},
\label{eq:c3-lora}
\end{equation}
式中：\symline{$\mathbf{W}$}{冻结主干权重；\\}
\hphantom{式中：}\symline{$\Delta\mathbf{W}$}{低秩增量参数；\\}
\hphantom{式中：}\symline{$r$}{LoRA秩；\\}
\hphantom{式中：}\symline{$\alpha$}{缩放系数；\\}
\hphantom{式中：}\symline{$\mathbf{A},\mathbf{B}$}{可训练低秩矩阵。}

\topichead{GA-LLM训练与推理伪代码}
\begin{tcolorbox}[
  breakable,
  colback=PromptBg,
  colframe=PromptBorder,
  boxrule=0.6pt,
  arc=2pt,
  left=8pt,right=8pt,top=6pt,bottom=6pt,
  title={Algorithm 3-1  GA-LLM Branch with GCIM/PAM Injection}
]
\texttt{Input: trajectory batch $\mathcal{B}$, structure embeddings $\mathbf{e}_{poi}$, coord set $\mathbf{g}$}\\
\texttt{Output: Top-$K$ prediction list for each trajectory}\\
\texttt{1: Build event-text sequence $\mathbf{X}_{traj}$ from $\mathcal{B}$}\\
\texttt{2: Compute $\mathbf{E}_{gps}\leftarrow$ GCIM($\mathbf{g}$), $\mathbf{E}_{poi}\leftarrow$ PAM($\mathbf{e}_{poi}$)}\\
\texttt{3: Fuse input by Eq.(\ref{eq:c3-input-fusion}) to obtain $\mathbf{H}^{(0)}$}\\
\texttt{4: For each Transformer layer: apply Eq.(\ref{eq:c3-mha}) and Eq.(\ref{eq:c3-ffn})}\\
\texttt{5: Compute next-token distribution by Eq.(\ref{eq:c3-next-token})}\\
\texttt{6: Train with Eq.(\ref{eq:c3-lgen}) and LoRA update Eq.(\ref{eq:c3-lora})}\\
\texttt{7: In inference, run beam search and return Top-$K$ POI IDs}
\end{tcolorbox}

推理阶段采用 beam search 直接生成候选并输出 Top-$K$。在本文设定中，\bluebf{不引入候选约束解码、不引入在线重排序、不引入TSPM推理打分}，即由GA-LLM单路完成最终预测输出。

\subsubsection{模块到研究问题的对应关系}
为保证“方法提出即有验证对象”，本文将核心模块与RQ映射如表\ref{tab:c3-rq-map}。

\begin{table}[htbp]
    \cThreeTblStyle
    \bitablecaption{模块设计与研究问题映射}{Mapping between module design and research questions.}
    \label{tab:c3-rq-map}
    \begin{tabular}{P{0.22\textwidth}P{0.44\textwidth}P{0.24\textwidth}}
        \toprule
        模块 & 主要目标 & 对应RQ \\
        \midrule
        TSDG + 时间分槽 & 捕获时段异质迁移 & RQ2 \\
        双向转移建模 & 降低方向偏置、提升下一跳稳定性 & RQ2, RQ4 \\
        动态图权重 & 强化复杂邻域区分能力 & RQ2, RQ4 \\
        GCIM & 提升地理一致性、抑制远跳错误 & RQ3, RQ4 \\
        PAM & 注入POI转移先验，改善缺失目标推断 & RQ3, RQ4 \\
        LoRA协同训练 & 平衡性能与训练成本 & RQ5 \\
        GA-LLM直推机制 & 在保持时延可控前提下输出稳定候选排序 & RQ1, RQ5 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{对齐与协同训练策略}
\subsection{两阶段训练流程}
阶段一冻结LLM主体，仅训练GCIM/PAM与映射层，建立稳定对齐；阶段二采用LoRA微调注意力层，联合优化序列与生成目标：
\begin{equation}
\mathcal{L}_{total}=\lambda_1\mathcal{L}_{gen}+\lambda_2\mathcal{L}_{geo}+\lambda_3\mathcal{L}_{align}+\lambda_4\mathcal{L}_{TSPM}.
\label{eq:c3-ltotal}
\end{equation}
式中：\symline{$\mathcal{L}_{total}$}{协同训练总损失；\\}
\hphantom{式中：}\symline{$\mathcal{L}_{gen}$}{生成任务损失；\\}
\hphantom{式中：}\symline{$\mathcal{L}_{geo}$}{地理一致性损失；\\}
\hphantom{式中：}\symline{$\mathcal{L}_{align}$}{跨空间对齐损失；\\}
\hphantom{式中：}\symline{$\mathcal{L}_{TSPM}$}{小模型结构损失；\\}
\hphantom{式中：}\symline{$\lambda_1,\lambda_2,\lambda_3,\lambda_4$}{对应权重系数。}

\topichead{训练稳定性策略}
联合训练中，不同损失尺度差异会导致优化不稳定。本文采用以下策略缓解：先进行warm-up对齐训练，再逐步提高联合损失权重；对梯度范数进行裁剪；采用早停与验证集监控防止过拟合。上述策略在不增加模型复杂度的情况下提升了收敛稳定性。
对应图\ref{fig:c3-framework}的训练路径，阶段一主要优化“结构流$\rightarrow$语义流”的对齐通道（GCIM/PAM），阶段二在GA-LLM任务目标上进行参数高效微调，避免一开始就全链路联训导致梯度相互干扰。

\subsection{耦合路径与参数更新机制}
尽管推理阶段仅保留GA-LLM单路输出，训练阶段仍通过 $\mathcal{L}_{align}$ 与 $\mathcal{L}_{geo}$ 把结构先验注入语义空间。具体耦合路径为：TSPM分支产生结构表示（如 $\boldsymbol{\xi}_{seq}$、POI结构嵌入）；PAM将其投影为语义空间向量 $\mathbf{E}_{poi}$；GCIM提供地理编码 $\mathbf{E}_{gps}$；最终在统一生成目标下更新GA-LLM侧可训练参数。该流程实质上是“训练期对齐注入”，而非“推理期分数融合”。

\begin{table}[htbp]
    \cThreeTblStyle
    \bitablecaption{两阶段训练中的参数冻结与更新策略}{Parameter freezing and updating strategy in two-stage training.}
    \label{tab:c3-stage-update}
    \begin{tabular}{P{0.15\textwidth}P{0.24\textwidth}P{0.24\textwidth}P{0.25\textwidth}}
        \toprule
        阶段 & 主要优化目标 & 更新参数集合 & 冻结参数集合 \\
        \midrule
        Stage-1 对齐预热 & $\mathcal{L}_{geo}+\mathcal{L}_{align}$ & GCIM投影层、PAM投影层、对齐映射层 & LLM主干参数、TSPM主干参数 \\
        Stage-2 协同训练 & $\mathcal{L}_{gen}+\lambda_2\mathcal{L}_{geo}+\lambda_3\mathcal{L}_{align}+\lambda_4\mathcal{L}_{TSPM}$ & LoRA适配参数、GCIM/PAM参数、对齐映射层、TSPM分支参数 & LLM全参数主干（非LoRA部分） \\
        \bottomrule
    \end{tabular}
\end{table}

\topichead{梯度回传说明}
Stage-2 中，$\mathcal{L}_{gen}$ 与 $\mathcal{L}_{align}$ 的梯度回传到 LoRA、GCIM 与 PAM；$\mathcal{L}_{TSPM}$ 梯度仅回传到 TSPM 分支；共享的是“对齐后的表示空间与训练目标”，而非在线推理分数。

\subsection{训练流程说明（实现视角）}
为便于复现实验，协同训练可归纳为以下步骤：
\begin{enumerate}
    \item 构建轨迹样本并并行生成序列流、结构流与语义流输入；
    \item 在阶段一中最小化 $\mathcal{L}_{geo}+\mathcal{L}_{align}$，得到稳定空间映射；
    \item 在阶段二中引入 $\mathcal{L}_{gen}$ 与 $\mathcal{L}_{TSPM}$，执行联合优化；
    \item 每个epoch后在验证集监控 Acc@1、Acc@5、MRR@5 与平均地理误差；
    \item 保存最优checkpoint并输出可复现实验配置。
\end{enumerate}

\subsection{推理机制}
推理时，先由GCIM与PAM将地理与转移信息注入提示，再由LLM输出候选分布并生成最终Top-$K$结果。
这一路径与图\ref{fig:c3-framework}的语义分支箭头对应：输入轨迹经编码注入后由GA-LLM直接完成下一POI预测。TSPM分支在本文中用于并行机制验证与对照分析，不参与该推理路径的最终输出。

\subsection{分支并行与结果对照}
本文采用“分支并行、结果对照”的组织方式：TSPM分支用于验证时空结构机制，GA-LLM分支用于验证地理注入与语义推理机制。最终报告分别基于各自分支的独立输出，不进行显式分数融合或在线重排序。

\subsection{关键超参数与默认配置}
结合前期实验设置\cite{liu2025tspm,liu2026gallm}，核心配置如表\ref{tab:c3-hparams}。其中大模型侧采用统一长上下文微调设置，小模型侧采用与TSPM一致的搜索区间并在验证集确定最优值。

\begin{table}[htbp]
    \cThreeTblStyle
    \bitablecaption{关键超参数与默认配置}{Key hyperparameters and default settings.}
    \label{tab:c3-hparams}
    \begin{tabular}{P{0.23\textwidth}P{0.18\textwidth}P{0.47\textwidth}}
        \toprule
        参数 & 默认值 & 说明 \\
        \midrule
        基座LLM & Llama-2-7b-longlora-32k & 大模型主干\cite{liu2026gallm} \\
        学习率（LLM侧） & $2\times 10^{-5}$ & 常数学习率，20步warm-up \\
        训练轮数（LLM侧） & 3 epochs & 各数据集统一设置 \\
        最大序列长度 & 32768 & 支持长轨迹输入 \\
        Batch Size / GPU & 1 & 与长上下文显存预算匹配 \\
        Quadkey层级 $L$ & 25 & 城市场景下兼顾精度与效率 \\
        时间槽数量 $z$ & 验证集选择 & 典型候选为 $[4,6,8,12]$ \\
        Beam size $b$ & 验证集选择 & 控制生成式解码分支数 \\
        输出Top-$K$ & 验证集选择 & 生成结果截断规模 \\
        LoRA秩 $r$ & 验证集选择 & 控制可训练参数规模 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{复杂度与可扩展性讨论}
训练成本主要来自三部分：TSPM动态图更新、LLM前向计算与跨模型对齐。相较于全参数微调，LoRA将可训练参数控制在低秩子空间，显著降低显存与训练时间开销。需要强调的是，推理阶段仅采用GA-LLM语义直推；TSPM仅用于离线对照、机制验证与诊断分析，不参与线上推理输出。

\subsubsection{时间复杂度分析}
设批大小为 $B$，序列长度为 $L$，隐藏维度为 $d$，动态图平均邻居数为 $K_n$，LLM层数为 $H$。则主要计算量可近似写为：
\begin{enumerate}
    \item {\heiti\addCJKfontfeatures{AutoFakeBold=2}TSPM编码：}$\mathcal{O}(B\cdot L\cdot d^2 + B\cdot L\cdot K_n\cdot d)$；
    \item {\heiti\addCJKfontfeatures{AutoFakeBold=2}GCIM/PAM注入：}$\mathcal{O}(B\cdot L\cdot d^2)$；
    \item {\heiti\addCJKfontfeatures{AutoFakeBold=2}LLM前向（LoRA）：}$\mathcal{O}(B\cdot L\cdot H\cdot d^2)$（主导项）。
\end{enumerate}
因此总体瓶颈仍在LLM前向，参数高效微调的价值在于显著减少反向传播所需的可训练参数与显存开销。

\subsubsection{空间复杂度分析}
空间开销由三部分组成：模型参数、激活缓存与动态图索引。相较全参微调，LoRA将可训练参数限制在低秩矩阵，显著降低优化器状态占用；动态图仅保留Top-$K_n$邻居，避免全图稠密存储。该组合使模型更易在单机多卡或中等算力环境下训练。

\subsubsection{可复现实现规范}
为提升结果复现概率，本文采用以下实践：
\begin{enumerate}
    \item 固定随机种子并记录数据切分索引；
    \item 保存每轮最优检查点与关键超参数配置；
    \item 将评估脚本与训练脚本解耦，避免指标实现漂移；
    \item 对核心实验进行多次重复并报告平均趋势而非单次最优值。
\end{enumerate}

\subsection{工程实现细节与落地策略}
除理论模块外，毕业论文还需要给出“可运行”的工程闭环。本节补充数据管线、训练调度与线上部署细节，以体现方法的可实施性。

\topichead{适用边界声明}
本节部分内容属于工程化可选策略，用于超大POI空间下的时延控制与服务兜底，不构成本文主实验的必要前提。本文主实验保持 GA-LLM 直接生成 Top-$K$ 的推理口径，不采用 TSPM 召回或在线约束重排。

\subsubsection{数据管线与样本构造}
\keytag{轨迹切片规则}\quad
本文统一采用前缀预测范式：给定用户轨迹 $ \mathcal{T}_{u}=\{x_1,\ldots,x_n\}$，构造监督样本为：
\begin{equation}
(\{x_1,\ldots,x_t\},x_{t+1}),\quad t\in[1,n-1].
\end{equation}
式中：\symline{$\{x_1,\ldots,x_t\}$}{历史前缀轨迹；\\}
\hphantom{式中：}\symline{$x_{t+1}$}{监督目标下一点；\\}
\hphantom{式中：}\symline{$t\in[1,n-1]$}{切片索引范围。}
该范式与真实推荐流程一致，可避免后缀信息泄露，并便于在离线与在线阶段共享样本定义。

\keytag{时间槽映射}\quad
对时间戳先进行本地时区对齐，再映射为时间槽索引。为减少节律冲突，工作日与休息日使用统一分槽规则，并附加二值指示位。该设计可在不增加大量参数的情况下增强周内与周末差异建模能力。

\keytag{解码规模控制}\quad
主实验推理采用 beam search 生成并输出 Top-$K$，通过 Beam size 与 Top-$K$ 两个参数平衡时延与命中率；不引入候选约束解码、logit mask 或在线重排序。

\subsubsection{分支接口与调度机制}
\topichead{TSPM分支接口}
TSPM分支输入由 POI 索引序列、时间槽序列与用户索引组成，输出包括候选打分与中间结构表示。该分支主要用于验证时空结构机制与提供对照结果，不直接改写GA-LLM推理输出。

\topichead{GCIM/PAM流水线}
GCIM执行“坐标归一化--层级编码--频域编码--投影融合”；PAM执行“结构嵌入读取--线性投影--语义空间对齐”。两条流水线在训练时共享批内样本索引，可减少跨模块数据搬运开销。

\topichead{动态图更新策略}
本文优先采用“按epoch离线更新图权重”的稳定策略，而非按batch增量更新。前者虽牺牲部分即时性，但可显著减少训练震荡并提升复现实验一致性，更适合作为论文主结果设置。

\subsubsection{两阶段训练的工程化约束}
阶段一仅优化对齐相关参数，目标是建立跨空间可读表示；阶段二引入任务损失执行联合优化。为保证稳定性，本文采用以下调度约束：对齐阶段较大学习率、联合阶段较小学习率；按验证集 Acc@1 与 MRR@5 双指标早停；保存“最优检查点 + 最近检查点”双副本，便于回滚与复盘。

\subsubsection{推理服务化与异常回退}
在线部署时，主链路保持“GA-LLM直推”，并通过 Beam size、Top-$K$ 与提示长度控制时延。当出现地理字段缺失、模板解析失败或模型超时时，系统回退到基础语义模板与规则过滤路径以保证服务可用性。

\subsubsection{实施清单}
为保证答辩阶段能够展示完整工程闭环，交付物按表\ref{tab:c3-impl-checklist}统一准备。

\begin{table}[htbp]
    \cThreeTblStyle
    \bitablecaption{方法实现交付清单}{Implementation delivery checklist of the proposed method.}
    \label{tab:c3-impl-checklist}
    \begin{tabular}{P{0.26\textwidth}P{0.62\textwidth}}
        \toprule
        交付项 & 说明 \\
        \midrule
        数据清洗与切分脚本 & 固定过滤规则、切分索引与版本哈希，确保实验可复验 \\
        训练配置文件 & 记录学习率、损失权重、随机种子、LoRA参数与时间槽设置 \\
        评估脚本与指标单测 & 统一Acc/MRR/NDCG计算逻辑，避免实现漂移 \\
        模型检查点规范 & 保留最优与最近检查点，支持恢复训练与结果回溯 \\
        日志与可视化面板 & 监控损失曲线、梯度范数与验证指标，支撑异常定位 \\
        部署回退策略文档 & 定义失败条件、回退路径与服务降级流程 \\
        \bottomrule
    \end{tabular}
\end{table}

\section{方法对照与讨论}
为突出本文协同路线的必要性，表\ref{tab:c3-capability-compare}从“时间异质建模、地理连续约束、转移先验注入、跨城泛化、部署成本可控”等维度对比代表方法能力边界。可以看出，单一路线通常只能覆盖局部能力，而协同设计能够在多个关键维度上同时满足要求。

\begin{table}[htbp]
    \cThreeTblStyle
    \bitablecaption{代表方法能力边界对照（“\ding{51}”表示具备主能力）}{Capability boundary comparison of representative methods (\ding{51} indicates core capability).}
    \label{tab:c3-capability-compare}
    \begin{tabular}{P{0.18\textwidth}ccccc}
        \toprule
        方法 & 时间异质 & 地理连续 & 转移先验 & 跨城泛化 & 成本可控 \\
        \midrule
        PRME / FPMC &  &  & \YesMark &  & \YesMark \\
        ST-RNN / STAN & \YesMark &  &  &  & \YesMark \\
        GETNext / STHGCN & \YesMark & \YesMark & \YesMark &  &  \\
        LLM4POI &  &  &  & \YesMark &  \\
        TSPM & \YesMark & \YesMark & \YesMark &  & \YesMark \\
        GA-LLM & \YesMark & \YesMark & \YesMark & \YesMark & \YesMark \\
        本文协同框架 & \YesMark & \YesMark & \YesMark & \YesMark & \YesMark \\
        \bottomrule
    \end{tabular}
\end{table}

该对照表对应两个结论：第一，传统方法和纯LLM方法的短板具有互补性；第二，协同框架并非“叠加模块”，而是围绕关键能力缺口做最小闭环补齐。第\ref{chap:exp}章将通过RQ1--RQ5逐项验证这种能力补齐是否转化为可测量收益。

\section{本章小结}
本章完成了协同方法的结构化设计。首先构建了统一框架并明确双分支信息流；其次在小模型侧设计TSDG、双向转移与动态图权重机制，以增强时空结构表达；再次在大模型侧设计GCIM与PAM，以注入地理连续性与POI转移先验；最后给出两阶段训练与推理流程，并讨论复杂度与可扩展性。新增的符号表、模块-RQ映射与参数配置表使方法定义更具可复现性，也为下一章按RQ组织的实验验证提供了明确的模块级假设与实现接口。
